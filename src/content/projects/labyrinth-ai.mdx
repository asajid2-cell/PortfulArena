---
title: "Labyrinth AI Trainer"
summary: "Interactive playground that teaches teams how to prompt and evaluate AI agents using arena metaphors."
problem: "New adopters needed a playful way to understand constraints, failure modes, and evaluation strategies without reading dense docs."
insight: "Gamifying the learning flow with spinning discs and clash scenarios made experimentation less intimidating."
approach: "Astro + MDX for the curriculum, with React islands handling the AI prompt simulations and physics-driven feedback."
outcome: "Workshops cut onboarding time by half, and 300+ participants shared replays within the first month."
role: "Experience engineer"
category: "Learning"
stack:
  - Astro
  - OpenAI API
  - Supabase
  - Tailwind CSS
  - Framer Motion
metrics:
  - label: "Participants"
    value: "310 in month one"
  - label: "Completion rate"
    value: "92%"
links:
  - label: "Workshop kit"
    href: "https://example.com/labyrinth-kit"
featured: false
order: 3
publishedAt: "2024-03-21"
---

## Why it works

Participants launch tops that represent agent strategies. They collide with scenario discs, sparking visual feedback that explains trade-offs and risk levels.

## My role

- Built the interactive clash visualizer with velocity decay and neon bloom states
- Integrated workshop scoring so teams could compare their launches
- Automated content publishing via MDX collections in Astro

## Looking ahead

We are shipping a self-serve portal with save states and exportable recaps next.
